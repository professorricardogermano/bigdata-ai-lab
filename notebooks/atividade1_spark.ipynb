{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atividade 1: Introdução ao PySpark - Transformações e Ações\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Esta atividade tem como objetivo familiarizar os alunos com o ambiente Spark e as operações básicas de **transformações** e **ações** utilizando PySpark (Python API para Spark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inicialização da SparkSession\n",
    "\n",
    "A `SparkSession` é o ponto de entrada para programar Spark com a API de DataFrames, Dataset e SQL. É fundamental para começar qualquer aplicação Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializa a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Atividade1PySpark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession inicializada com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregamento de Dados\n",
    "\n",
    "Vamos carregar um pequeno conjunto de dados de vendas. Considere que este arquivo `vendas.csv` estará disponível no seu ambiente Spark (pode ser via HDFS ou sistema de arquivos local da VM).\n",
    "\n",
    "**Crie um arquivo `vendas.csv` no seu ambiente com o seguinte conteúdo:**\n",
    "```csv\n",
    "ID_Venda,Produto,Categoria,Valor,Quantidade,Data_Venda\n",
    "1,Teclado,Eletronicos,150.00,1,2024-01-05\n",
    "2,Mouse,Eletronicos,75.50,2,2024-01-05\n",
    "3,Monitor,Eletronicos,800.00,1,2024-01-06\n",
    "4,Mochila,Acessorios,120.00,1,2024-01-07\n",
    "5,Fone de Ouvido,Eletronicos,200.00,1,2024-01-07\n",
    "6,Caderno,Papelaria,25.00,5,2024-01-08\n",
    "7,Caneta,Papelaria,5.00,10,2024-01-08\n",
    "8,Impressora,Eletronicos,450.00,1,2024-01-09\n",
    "9,Cadeira Gamer,Mobiliario,1200.00,1,2024-01-10\n",
    "10,Webcam,Eletronicos,180.00,1,2024-01-10\n",
    "```\n",
    "Guarde este arquivo em um local acessível dentro da sua VM, por exemplo, `/home/aluno/data/vendas.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o CSV para um DataFrame\n",
    "df_vendas = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"file:///home/aluno/data/vendas.csv\") # Use 'file:///' para caminhos locais\n",
    "\n",
    "print(\"Esquema do DataFrame:\")\n",
    "df_vendas.printSchema()\n",
    "\n",
    "print(\"Primeiras 5 linhas do DataFrame:\")\n",
    "df_vendas.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformações Comuns\n",
    "\n",
    "Transformações são operações que produzem um novo DataFrame sem disparar uma computação imediata (lazy evaluation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrando Dados (`filter` ou `where`)\n",
    "\n",
    "Vamos filtrar as vendas que são da categoria 'Eletronicos'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eletronicos = df_vendas.filter(df_vendas.Categoria == 'Eletronicos')\n",
    "print(\"Vendas de Eletrônicos:\")\n",
    "df_eletronicos.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecionando e Renomeando Colunas (`select`, `withColumnRenamed`)\n",
    "\n",
    "Vamos selecionar apenas 'Produto', 'Valor' e 'Categoria', e renomear 'Valor' para 'Preco_Unitario'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selecionado = df_vendas.select(\"Produto\", \"Valor\", \"Categoria\") \\\n",
    "                          .withColumnRenamed(\"Valor\", \"Preco_Unitario\")\n",
    "\n",
    "print(\"DataFrame com colunas selecionadas e renomeadas:\")\n",
    "df_selecionado.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adicionando Novas Colunas (`withColumn`)\n",
    "\n",
    "Vamos adicionar uma coluna `Valor_Total_Venda` (Valor * Quantidade)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_vendas_total = df_vendas.withColumn(\"Valor_Total_Venda\", col(\"Valor\") * col(\"Quantidade\"))\n",
    "\n",
    "print(\"DataFrame com 'Valor_Total_Venda':\")\n",
    "df_vendas_total.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ações Comuns\n",
    "\n",
    "Ações são operações que disparam a execução de uma computação e retornam um resultado para o driver ou escrevem em um sistema de armazenamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contar Linhas (`count`)\n",
    "\n",
    "Quantas vendas há no total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vendas = df_vendas.count()\n",
    "print(f\"Total de vendas: {total_vendas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coletar Dados (`collect`, `take`)\n",
    "\n",
    "Traga os resultados para o driver (use com cautela em grandes datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primeira_venda = df_vendas.take(1)\n",
    "print(\"Primeira venda (lista de Row objects):\")\n",
    "print(primeira_venda)\n",
    "\n",
    "todos_produtos = df_vendas.select(\"Produto\").collect()\n",
    "print(\"Todos os produtos vendidos:\")\n",
    "for row in todos_produtos:\n",
    "    print(row['Produto'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregações (`groupBy`, `agg`)\n",
    "\n",
    "Calcular o valor total de vendas por categoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df_vendas_por_categoria = df_vendas.groupBy(\"Categoria\").agg(sum(\"Valor\").alias(\"Total_Vendido\"))\n",
    "\n",
    "print(\"Total vendido por Categoria:\")\n",
    "df_vendas_por_categoria.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios Práticos\n",
    "\n",
    "Agora é a sua vez! Utilize o DataFrame `df_vendas` ou crie novos a partir dele para resolver os seguintes problemas:\n",
    "\n",
    "1.  **Qual o produto mais caro vendido?** (Dica: use `orderBy` e `limit`).\n",
    "2.  **Liste todas as vendas realizadas em 2024-01-08.**\n",
    "3.  **Calcule a quantidade total de itens vendidos por categoria.** (Dica: `groupBy` e `sum`).\n",
    "4.  **Crie um novo DataFrame que contenha apenas as vendas com `Quantidade` maior que 1 e adicione uma nova coluna `Preco_Unitario_Medio` (Valor / Quantidade).**\n",
    "\n",
    "## Finalização da SparkSession\n",
    "\n",
    "É uma boa prática parar a SparkSession ao final do seu script ou notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"SparkSession parada.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}